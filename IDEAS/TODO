


We need a way of making our data more anisitropic.

1:

first of all, we want a method that looks at all hard-negative candidates.
When we do this, we will want to compute square matrices of the embeddings.
after we do the filtering.

To do this, we will keep the distances of each embedding with each other, and store them during the embeddings computations.

2: I wish to test cagra's ability to index on the cloud, and test its retrieval ability. we should do this on a cheap pc.

3: Work alot on scoring systems, figure out what is best.


COMPUTE DISTANCE MATRICES!!!!!!!  Important.

We will keep the distance matrices of the embeddings.


================================================================

Once we have created our model, I wish to search using 20nn for each work, and create a dataset of
each work that is not being found on the encodings we have.

For each work object we do not find in our search through our whole index, we will take create a triplet from the 20nn
by searching for that specific work and creating a triplet from that work + two augmentations of it.

================================================================

We can go through our model, and for each work that isn't found on the 20 nn search, we will
actually just add them to a separate dataset, make triplets from them using knn-search,
and then fine-tune another encoder on those triplets, so we search via two encoders.




----------------------------------------------------------------

We have to do the following:

We have to figure out the perfect scoring system for total_score calculation.

What we are going to do is the following:

We will first construct models that have title + author + field + subfield

Then, we will have models that have title + author + field + subfield + topic + keywords

We want to make the full triplets parquet file and dataset, and then we want to make some others, such as these:

We want:

-----------------------------------------------------------------

We want the following:

We want in our works collection:

key_phrases
key_names

key_names will contain a list of all the first and last names of authors for the work, which are not initials.
key_phrases will contain a list of all the key phrases found in the abstract or title.

These will be used for meta-data filtering.

-----------------------------------------------------------------

Now, we will make a main triplets parquet file:

we expect the triplets file to be 5gb per 10 million triplets, and so around 50gb loaded up as a polars dataframe.

This means, at most we want around 300 million triplets. This will be difficult to manage and we will have to do a lot
of calculations to manage it, and keep memory down.

We want fine-tuning datasets as well:

One for authors + field + subfield
One for titles + field + subfield

One for curriculum learning, i.e the ones with high total scores. This needs to be very balanced.

One for each domain as well..

We should do this in a way that is efficient. The way we will do this is by looking at field, source, and augmentation_type.

# Select and order final columns
final_columns = [
    'anchor', 'positive', 'negative',
    'anchor_string', 'positive_string', 'negative_string',
    'total_score_pos', 'total_score_neg',
    'z_score_pos', 'z_score_neg',
    'max_pos_neg_distance',
    'source_pos', 'source_neg',
    'augmentation_type_pos', 'augmentation_type_neg'
]


# Define all possible augmentations
augmentations = [
    ('full_title', lambda: title_string),
    ('full_title_field', lambda: f"{title_string} {field_string}"),
    ('author_field', lambda: f"{author_names[0] if author_names else ''} {field_string}"),
    ('all_authors_field', lambda: f"{' '.join(author_names)} {field_string}"),
    ('one_author_field_subfield',
     lambda: f"{author_names[0] if author_names else ''} {field_string} {subfield_string}"),
    (
    'two_authors_field_subfield', lambda: f"{' '.join(author_names[:2])} {field_string} {subfield_string}"),
    ('two_authors_field', lambda: f"{' '.join(author_names[:2])} {field_string}"),
    ('full_title_field_subfield', lambda: f"{title_string} {field_string} {subfield_string}"),
    ('all_authors_field_subfield', lambda: f"{' '.join(author_names)} {field_string} {subfield_string}"),
    ('field', lambda: field_string),
    ('field_subfield', lambda: f"{field_string} {subfield_string}"),
    ('top_unigram', lambda: top_unigrams[0] if top_unigrams else ''),
    ('top_two_unigrams', lambda: ' '.join(top_unigrams[:2]) if len(top_unigrams) >= 2 else ''),
    ('top_three_unigrams', lambda: ' '.join(top_unigrams[:3]) if len(top_unigrams) >= 3 else ''),
    ('top_unigram_field_subfield',
     lambda: f"{top_unigrams[0] if top_unigrams else ''} {field_string} {subfield_string}"),
    ('authors_no_initials', lambda: ' '.join([name for name in author_names if len(name) > 2]))
]

These are the augmentation types. (please just list the keys).

and sources are:

'source': 'common_title_works',
'source': 'works_common_authors',
'source': 'works_augmented_data',

based off this information, create a method that lists the source and augmentation_types that get a work filtered into the right fine-tuning dataset

One for authors + field + subfield
One for titles + field + subfield







